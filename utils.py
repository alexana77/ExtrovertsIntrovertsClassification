{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# %% [code]\n# %% [code]\n# %% [code]\n# %% [code]\n# %% [code]\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-11T09:48:58.658268Z\",\"iopub.execute_input\":\"2025-08-11T09:48:58.658600Z\",\"iopub.status.idle\":\"2025-08-11T09:48:58.665302Z\",\"shell.execute_reply.started\":\"2025-08-11T09:48:58.658572Z\",\"shell.execute_reply\":\"2025-08-11T09:48:58.664205Z\"}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\nimport numpy as np\nimport pandas as pd\nimport json\nimport joblib\nfrom typing import List, Tuple\nimport shap\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, jaccard_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom xgboost import XGBClassifier\nfrom scipy import stats, sparse\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nLABEL_MAP = {'Introverts': 1, 'Extroverts': 0}\nINV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\nPOS_CLASS = 1  # always Introverts\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-11T09:48:58.666746Z\",\"iopub.execute_input\":\"2025-08-11T09:48:58.667043Z\",\"iopub.status.idle\":\"2025-08-11T09:48:58.695740Z\",\"shell.execute_reply.started\":\"2025-08-11T09:48:58.667019Z\",\"shell.execute_reply\":\"2025-08-11T09:48:58.694765Z\"}}\n\n# Association measure\ndef cramers_v(x: pd.Series, y: pd.Series) -> float:\n    \"\"\"\n    Calculate Cramér's V statistic for measuring the strength of association between\n    two categorical variables. Bias‑correction (and zero‑division guard) is done.\n    \n    In small contingency tables, the raw phi^2 statistic tends to overestimate association strength. \n    That’s because:\n            - chi-squared test is sensitive to sample size and table dimensions.\n            - small tables can produce large phi^2 values even when the actual association is weak.    \n\n    Parameters\n    ----------\n    x: first categorical variable\n    y: second categorical variable\n\n    Returns\n    -------\n    0 is no association,  1 is a perfect association\n    \"\"\"\n    cont_table = pd.crosstab(x, y)\n    chi2 = stats.chi2_contingency(cont_table, correction=False)[0]\n    n = cont_table.values.sum()\n    if n == 0:\n        return 0.0\n    phi2 = chi2 / n\n    r, k = cont_table.shape\n    # Bias correction\n    phi2_corr = max(0, phi2 - (k-1)*(r-1)/(n-1))\n    k_corr = k - (k-1)**2/(n-1)\n    r_corr = r - (r-1)**2/(n-1)\n    denom = min((k_corr-1), (r_corr-1)) #if min(k-1, r-1)==0, we’ll get division by zero\n    if denom <= 0:\n        return 0.0\n    return float(np.sqrt(phi2_corr / denom))\n    \n\n# Class balance check\ndef check_class_distribution(y_train: pd.Series, y_test: pd.Series, tolerance=0.02)-> None:\n    \"\"\"\n    Validate class distribution consistency between training and test sets.\n\n    This function compares the relative frequency of each class in the training\n    and test datasets. If the difference in proportions for any class exceeds\n    the specified `tolerance`, it raises a ValueError. Otherwise, it confirms\n    that distributions are similar.\n\n    Parameters\n    ----------\n    y_train : target variable for the training dataset\n    y_test  : target variable for the test dataset\n    tolerance : maximum allowable absolute difference in class proportions between \n        training and test sets\n    \"\"\"\n    \n    # Calculate class distributions\n    classes = sorted(set(y_train.unique()) | set(y_test.unique()))\n    train_dist = y_train.value_counts(normalize=True).reindex(classes, fill_value=0.0)\n    test_dist = y_test.value_counts(normalize=True).reindex(classes, fill_value=0.0)\n    \n    # Check that each class proportion in test set is within tolerance of train set\n    for cls in train_dist.index:\n        diff = abs(train_dist[cls] - test_dist[cls])\n        if diff > tolerance:\n            raise ValueError(\n                f\"Class distribution mismatch exceeds tolerance of {tolerance}:\\n\"\n                f\"Train dist: {train_dist.to_dict()}\\n\"\n                f\"Test dist:  {test_dist.to_dict()}\\n\"\n                f\"Difference: {diff}\"\n        )\n    else:\n        print(\"Class distributions are similar within tolerance.\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-11T09:48:58.748437Z\",\"iopub.execute_input\":\"2025-08-11T09:48:58.748779Z\",\"iopub.status.idle\":\"2025-08-11T09:48:58.772547Z\",\"shell.execute_reply.started\":\"2025-08-11T09:48:58.748747Z\",\"shell.execute_reply\":\"2025-08-11T09:48:58.771518Z\"}}\n\n# Training & prediction\ndef train_and_test(pipeline: Pipeline,X_train: pd.DataFrame,X_test: pd.DataFrame,y_train: pd.Series) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Fit a pipeline on training data and generate predictions and probabilities on test data.\n\n    Parameters\n    ----------\n    pipeline : a scikit-learn compatible pipeline with a fitted classifier that supports `predict()` and `predict_proba()` methods\n    X_train : training feature set\n    X_test : test feature set\n    y_train : target values for training\n\n    Returns\n    -------\n    y_pred : predicted class labels for X_test\n    y_prob : predicted class probabilities for the positive class (class=1) for X_test\n    \"\"\"\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n\n    classes_ = list(pipeline.named_steps['classifier'].classes_)\n    pos_idx = classes_.index(POS_CLASS)\n    y_prob = pipeline.predict_proba(X_test)[:, pos_idx]\n    return y_pred, y_prob\n    \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-11T09:48:58.773708Z\",\"iopub.execute_input\":\"2025-08-11T09:48:58.774006Z\",\"iopub.status.idle\":\"2025-08-11T09:48:58.795721Z\",\"shell.execute_reply.started\":\"2025-08-11T09:48:58.773983Z\",\"shell.execute_reply\":\"2025-08-11T09:48:58.794762Z\"}}\n\n# Evaluation\ndef evaluate(y_test: pd.Series, y_pred: np.ndarray, y_prob: np.ndarray) -> None:\n    \"\"\"\n    Evaluate classification model performance using multiple metrics.\n\n    Parameters\n    ----------\n    y_test : true class labels for the test set.\n    y_pred : predicted class labels from the model.\n    y_prob :predicted probabilities for the positive class (class=1).\n\n    Prints\n    ------\n    - Classification report (precision, recall, f1-score, support)\n    - Confusion matrix\n    - ROC AUC score\n    - Jaccard score (binary)\n    \"\"\"\n    \n    labels = np.sort(np.unique(np.r_[y_test, y_pred]))\n    target_names = [INV_LABEL_MAP.get(l, str(l)) for l in labels]\n    \n    cm = confusion_matrix(y_test, y_pred, labels=labels)\n    cm_df = pd.DataFrame(\n        cm, \n        index=[f\"Actual {INV_LABEL_MAP[l]}\" for l in labels], \n        columns=[f\"Pred {INV_LABEL_MAP[l]}\" for l in labels]\n    )\n       \n    print(f\"\\033[1mClassification Report:\\033[0m\")\n    print(classification_report(\n        y_test, y_pred,\n        labels=labels,\n        target_names=target_names,\n        zero_division=0\n    ))\n    \n    print(f\"\\033[1mConfusion Matrix:\\033[0m\\n{cm_df}\\n\")\n    \n    print(f\"\\033[1mROC AUC Score:\\033[0m\\n (pos={INV_LABEL_MAP[POS_CLASS]}):{roc_auc_score(y_test, y_prob):.3f}\\n\")\n    print(f\"\\033[1mJaccard Score:\\033[0m\\n (pos={INV_LABEL_MAP[POS_CLASS]}):{jaccard_score(y_test, y_pred, pos_label=POS_CLASS):.3f}\\n\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-08-11T09:48:58.696658Z\",\"iopub.execute_input\":\"2025-08-11T09:48:58.697012Z\",\"iopub.status.idle\":\"2025-08-11T09:48:58.720822Z\",\"shell.execute_reply.started\":\"2025-08-11T09:48:58.696976Z\",\"shell.execute_reply\":\"2025-08-11T09:48:58.719772Z\"}}\n\n# Feature importance\ndef calc_and_print_xgb_feat_importance(pipeline: Pipeline, top_num=15) -> pd.DataFrame:\n    \"\"\"\n    Calculate and display the top XGBoost feature importances from a fitted pipeline.\n\n    Parameters\n    ----------\n    pipeline : a trained scikit-learn pipeline containing:\n        - 'preprocessor': a transformer with get_feature_names_out() method\n        - 'classifier': an XGBoost model with feature_importances_ attribute\n\n    Returns\n    -------\n    feature_importance_df : DataFrame sorted by importance (descending) with columns: 'Feature' and 'Importance'   \n    \"\"\"\n        \n    xgb_model = pipeline.named_steps['classifier']\n    feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n    importances = xgb_model.feature_importances_  \n    feature_importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances.round(4)\n    }).sort_values(by='Importance', ascending=False)\n    \n    print(\"Top Feature Importances:\")\n    print(feature_importance_df.head(top_num))\n\n    return feature_importance_df\n\n\n# %% [code]\n\n# SHAP calculations\ndef calc_shap(pipeline: Pipeline, X: pd.DataFrame) -> Tuple[shap.Explainer, np.ndarray, shap.Explanation, List[str]]:\n    \"\"\"\n    Compute SHAP values for a fitted pipeline containing a preprocessor and an XGBoost classifier.\n\n    Parameters\n    ----------\n    pipeline : a fitted Pipeline with steps\n        - 'preprocessor': transformer supporting transform() and preferably get_feature_names_out()\n        - 'classifier'  : fitted XGBoost model (scikit-learn API)\n    X : raw input features (unprocessed).\n\n    Returns\n    -------\n    explainer : SHAP explainer built for the fitted classifier.\n    X_transformed : feature matrix after preprocessing (numeric, possibly sparse).\n    shap_values : SHAP values for X_transformed.\n    feature_names : transformed feature names (matches columns of X_transformed).\n    \"\"\"\n    if not hasattr(pipeline, \"named_steps\"):\n        raise ValueError(\"pipeline must be a fitted scikit-learn Pipeline with named steps.\")\n\n    if \"preprocessor\" not in pipeline.named_steps or \"classifier\" not in pipeline.named_steps:\n        raise ValueError(\"pipeline must contain 'preprocessor' and 'classifier' steps.\")\n\n    preprocessor = pipeline.named_steps[\"preprocessor\"]\n    classifier = pipeline.named_steps[\"classifier\"]\n    X_transformed = preprocessor.transform(X)\n\n    if sparse.issparse(X_transformed):\n        X_transformed = X_transformed.toarray()\n    \n    if hasattr(preprocessor, \"get_feature_names_out\"):\n        feature_names = list(preprocessor.get_feature_names_out())\n    else:\n        # Fallback: try to build names from individual transformers\n        feature_names = []\n        if hasattr(preprocessor, \"transformers_\"):\n            for _, transformer, cols in preprocessor.transformers_:\n                if hasattr(transformer, \"get_feature_names_out\"):\n                    names = transformer.get_feature_names_out(cols)\n                    feature_names.extend(names)\n                else:\n                    # If  can't get names, append the raw column names\n                    feature_names.extend(list(cols) if hasattr(cols, \"__iter__\") else [cols])\n\n    # Build SHAP explainer and compute values\n    explainer = shap.TreeExplainer(classifier.get_booster())\n    shap_values = explainer(X_transformed)\n\n    return explainer, X_transformed, shap_values, feature_names\n    \n# %% [code]\ndef plot_summary_shap(X_processed:np.ndarray, shap_values_obj:shap.Explanation, transformed_feature_names:list) -> None: \n    \"\"\"\n    Create a labeled bar plot of mean(|SHAP|) per feature (descending)\n\n    Parameters:\n    -----------\n    X_processed : transformed features after preprocessing pipeline\n    shap_values_obj : SHAP values object from explainer\n    transformed_feature_names : list of feature names after transformation\n\n    Returns\n    -------\n    summary_shap_df : DataFrame sorted by shap value (descending) with columns: 'Feature' and 'Importance'   \n    \n    \"\"\"\n    shap_arr = shap_values_obj[-1] if isinstance(shap_values_obj, list) else shap_values_obj\n    plt.clf()\n    shap.summary_plot(shap_arr, features=X_processed, feature_names=transformed_feature_names, plot_type=\"bar\", show=False)\n    \n    mean_abs_shap = np.abs(shap_arr.values).mean(axis=0)\n    order  = np.argsort(mean_abs_shap)\n    \n    ax = plt.gca()\n    for i, v in enumerate(mean_abs_shap[order]):\n        ax.text(v + 1e-3, i, f\"{v:.3f}\", va='center')\n    plt.tight_layout()\n    plt.show()\n    \n    return (pd.DataFrame({\"Feature\": np.array(transformed_feature_names)[order], \"|Mean SHAP|\": mean_abs_shap[order]})\n              .sort_values(\"|Mean SHAP|\", ascending=False))\n   \n\n# %% [code]\ndef plot_dependency_shap(X_processed:np.ndarray, shap_values_obj:shap.Explanation, transformed_feature_names:list, plot_feature='') -> None: \n    \"\"\"\n    Plot SHAP dependence for a single feature.\n\n    Parameters:\n    -----------\n    X_processed : transformed features after preprocessing pipeline\n    shap_values_obj : SHAP values object from explainer\n    transformed_feature_names : list of feature names after transformation\n    plot_feature : name of the feature to plot\n    \"\"\"\n    X_df = pd.DataFrame(X_processed, columns=transformed_feature_names)\n    \n    shap.dependence_plot(\n        plot_feature,  \n        shap_values_obj.values,\n        X_df,     \n        feature_names=transformed_feature_names, \n        show=False\n    )\n    \n    plt.title(f\"SHAP Dependence Plot: {plot_feature}\")\n    plt.tight_layout()\n    plt.show()\n    \n\n# Save artifacts\ndef save_artifacts(suffix: str, pipeline, X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, y_test: pd.Series):\n    '''\n    Save pipeline, XGBoost model, datasets, and metadata for later reuse.\n    '''\n\n    # Extract pipeline components\n    preproc = pipeline.named_steps.get('preprocessor', None)\n    clf = pipeline.named_steps.get('classifier', pipeline)\n\n    # Feature names after preprocessing\n    try:\n        feature_names = list(preproc.get_feature_names_out())\n    except Exception:\n        feature_names = []\n\n    # Save pipeline and classifier\n    joblib.dump(pipeline, f'{suffix}_xgboost_pipeline.joblib')\n\n    if hasattr(clf, 'feature_importances_'):\n        joblib.dump(clf, f'{suffix}_xgb_classifier.joblib')\n        try:\n            clf.get_booster().save_model(f'{suffix}_xgb_model.json')\n        except Exception:\n            clf.save_model(f'{suffix}_xgb_model.json')\n\n    # Save datasets\n    X_train.to_parquet(f'{suffix}_X_train.parquet', index=False)\n    X_test.to_parquet(f'{suffix}_X_test.parquet', index=False)\n    y_train.to_frame('target').to_parquet(f'{suffix}_y_train.parquet', index=False)\n    y_test.to_frame('target').to_parquet(f'{suffix}_y_test.parquet', index=False)\n\n    # Extract classifier parameters safely\n    def sanitize_params(params):\n        sanitized = {}\n        for k, v in params.items():\n            try:\n                json.dumps(v)  # test if serializable\n                sanitized[k] = v\n            except TypeError:\n                sanitized[k] = str(v)  # fallback: convert to string\n        return sanitized\n\n    # Save metadata\n    meta = {\n        'name': suffix,\n        'shapes': {\n            'X_train': list(X_train.shape),\n            'X_test': list(X_test.shape),\n            'y_train': int(y_train.shape[0]),\n            'y_test': int(y_test.shape[0]),\n        },\n        'feature_names': feature_names,\n        'classifier_params': sanitize_params(clf.get_params()) if hasattr(clf, 'get_params') else {},\n        'pipeline_path': f'{suffix}_xgboost_pipeline.joblib',\n        'classifier_path': f'{suffix}_xgb_classifier.joblib',\n        'model_path': f'{suffix}_xgb_model.json'\n    }\n\n    with open(f'{suffix}_metadata.json', 'w', encoding='utf-8') as f:\n        json.dump(meta, f, indent=2)\n\n    \nif __name__ == '__main__':\n    print('Demo run of all utils with Introverts=1 mapping')\n    np.random.seed(42)\n    df = pd.DataFrame({\n        \"feature_num1\": np.random.randn(100),\n        \"feature_num2\": np.random.randn(100) * 5,\n        \"feature_cat\": np.random.choice([\"A\", \"B\", \"C\"], size=100),\n        \"target\": np.random.choice([0, 1], size=100, p=[0.3, 0.7])  # 1=Introverts, 0=Extroverts\n    })\n\n    X = df.drop(columns=[\"target\"])\n    y = df[\"target\"]\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, stratify=y, test_size=0.3, random_state=42\n    )\n\n    numeric_features = [\"feature_num1\", \"feature_num2\"]\n    categorical_features = [\"feature_cat\"]\n\n    preprocessor = ColumnTransformer([\n        (\"num\", StandardScaler(), numeric_features),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n    ], verbose_feature_names_out=False)\n\n    pos_ratio = (y_train == POS_CLASS).mean()\n    scale_pos_weight = (1 - pos_ratio) / pos_ratio\n\n    pipeline = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", XGBClassifier(\n            eval_metric=\"logloss\", random_state=42,\n            scale_pos_weight=scale_pos_weight\n        ))\n    ])\n\n    # --- Tests of each function ---\n    # 1) Check class balance\n    check_class_distribution(y_train, y_test, 0.1)\n\n    # 2) Train & predict\n    y_pred, y_prob = train_and_test(pipeline, X_train, X_test, y_train)\n\n    # 3) Evaluate metrics\n    evaluate(y_test, y_pred, y_prob)\n\n    # 4) Feature importance\n    calc_and_print_xgb_feat_importance(pipeline)\n\n    # 5) SHAP calculations + plots\n    explainer, Xtr, shap_values, f_names = calc_shap(pipeline, X_train)\n    plot_summary_shap(Xtr, shap_values, f_names)\n    plot_dependency_shap(Xtr, shap_values, f_names, plot_feature=f_names[0])\n\n    # 6) Cramér’s V demo (association between categorical col and target)\n    cv = cramers_v(df[\"feature_cat\"], df[\"target\"])\n    print(f\"\\nCramér’s V(feature_cat, target) = {cv:.3f}\")\n\n    # 7) Save artifacts (pipeline, data, metadata)\n    save_artifacts(\"demo\", pipeline, X_train, X_test, y_train, y_test)\n    print(\"\\nArtifacts saved with prefix 'demo_'\")\n     \n","metadata":{"_uuid":"95d638d6-d944-4305-b9dc-30d8f121aa4e","_cell_guid":"09a077ae-b90a-4c06-a651-ab54764cac8c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}